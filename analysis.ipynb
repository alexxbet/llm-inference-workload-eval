{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import find_mean_coord, get_cdf, get_tick_positions, DATASETS_META, ROOT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "mpl.rcParams['ps.fonttype'] = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 1 (Dataset Characteristics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, meta in DATASETS_META.items():\n",
    "    with open(meta[\"path\"], \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    filtered = []\n",
    "    for pair in data:\n",
    "        input_tokens, output_tokens = pair\n",
    "\n",
    "        if isinstance(input_tokens, int):\n",
    "            input_tokens = [1000] * input_tokens\n",
    "        if isinstance(output_tokens, int):\n",
    "            output_tokens = [1000] * output_tokens\n",
    "        \n",
    "        input_len = len(input_tokens)\n",
    "        output_len = len(output_tokens)\n",
    "\n",
    "        # Filter out too long sequences.\n",
    "        if input_len + output_len < MAX_SEQ_LEN:\n",
    "            filtered.append((input_len, output_len))\n",
    "\n",
    "    DATASETS_META[name][\"requests\"] = filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_label = {\n",
    "    \"alpaca\": \"Alpaca\",\n",
    "    \"cnn_dailymail\": \"CNN DailyMail\",\n",
    "    \"dolly\": \"Dolly\",\n",
    "    \"sharegpt\": \"ShareGPT\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axis_title_size = 32\n",
    "label_size = 32\n",
    "legend_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 2\n",
    "fig, axs = plt.subplots(1, 2, figsize=[scale*6.4, 4.8], constrained_layout=True)\n",
    "\n",
    "ax = axs.flat[0]\n",
    "for name, meta in sorted(DATASETS_META.items()):\n",
    "    requests = meta[\"requests\"]\n",
    "    \n",
    "    # Plot line\n",
    "    x, y = get_cdf([req[0] for req in requests])\n",
    "\n",
    "    ax.plot(x, y, label=dataset_label[name], color=meta[\"color\"], linewidth=4)\n",
    "\n",
    "    # Plot mean values\n",
    "    x_mean, y_mean = find_mean_coord(x, y)\n",
    "\n",
    "    ax.plot(x_mean, y_mean, marker=\"o\", markersize=8, color=meta[\"color\"])\n",
    "\n",
    "# Plot median line\n",
    "xmin, xmax = ax.get_xlim()\n",
    "\n",
    "ax.hlines(y=0.5, xmin=xmin, xmax=xmax, colors=\"#999999\", ls=\"--\", linewidth=2,\n",
    "        alpha=0.8)\n",
    "\n",
    "ax.set_ylabel(\"Probability (%)\", size=axis_title_size)\n",
    "ax.set_yticks([0.0, 0.25, 0.5, 0.75, 1.0])\n",
    "ax.set_yticklabels([\"0 \", \"25\", \"50\", \"75\", \"100\"])\n",
    "ax.tick_params(axis=\"y\", labelsize=label_size)\n",
    "\n",
    "ax.set_xlabel(\"Input Length (token)\", size=axis_title_size)\n",
    "ax.set_xscale(\"log\", base=10, subs=[])\n",
    "ax.set_xticks([1, 10, 100, 1000])\n",
    "ax.set_xticklabels([\"\", \"10\", \"100\", \"1000\"])\n",
    "ax.tick_params(axis=\"x\", labelsize=label_size)\n",
    "\n",
    "ax = axs.flat[1]\n",
    "for name, meta in DATASETS_META.items():\n",
    "    requests = meta[\"requests\"]\n",
    "    \n",
    "    # Plot line\n",
    "    x, y = get_cdf([req[1] for req in requests])\n",
    "    \n",
    "    ax.plot(x, y, label=None, color=meta[\"color\"], linewidth=4)\n",
    "\n",
    "    # Plot mean values\n",
    "    x_mean, y_mean = find_mean_coord(x, y)\n",
    "\n",
    "    ax.plot(x_mean, y_mean, marker=\"o\", markersize=8, color=meta[\"color\"])\n",
    "\n",
    "# Plot median line\n",
    "xmin, xmax = ax.get_xlim()\n",
    "\n",
    "ax.hlines(y=0.5, xmin=xmin, xmax=xmax, colors=\"#999999\", ls=\"--\", linewidth=2,\n",
    "        alpha=0.8)\n",
    "\n",
    "ax.set_yticks([])\n",
    "\n",
    "ax.set_xlabel(\"Output Length (token)\", size=axis_title_size)\n",
    "ax.set_xscale(\"log\", base=10, subs=[])\n",
    "ax.set_xticks([1, 10, 100, 1000])\n",
    "ax.set_xticklabels([\"\", \"10\", \"100\", \"1000\"])\n",
    "ax.tick_params(axis=\"x\", labelsize=label_size)\n",
    "\n",
    "leg = fig.legend(\n",
    "    loc=\"upper center\", fontsize=legend_size, ncols=4,\n",
    "    bbox_to_anchor=(0.5, 1.25), columnspacing=1,\n",
    "    handlelength=1.2, handletextpad=0.5\n",
    ")\n",
    "\n",
    "for handle in leg.legend_handles:\n",
    "    handle.set_linewidth(7.5)\n",
    "file_path = os.path.join(ROOT_DIR, \"figures\", \"figure_1.pdf\")\n",
    "\n",
    "fig.savefig(file_path, dpi=300, bbox_inches=\"tight\", format=\"pdf\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 2 (Maximum Request Rate under SLO per Use Case, Maximum Throughput per Use Case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RES_DIR = \"./proc-outputs/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metadata import (\n",
    "    ONLINE_EXPERIMENTS_META_OPT_13B,\n",
    "    ONLINE_EXPERIMENTS_META_LLAMA_2_13B,\n",
    "    ONLINE_EXPERIMENTS_META_OPT_7B,\n",
    "    ONLINE_EXPERIMENTS_META_LLAMA_2_7B\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "DURATION = 600\n",
    "BLOCK_SIZE = 16\n",
    "N = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for online_experiments_meta, model in zip(\n",
    "    [ONLINE_EXPERIMENTS_META_OPT_13B, ONLINE_EXPERIMENTS_META_LLAMA_2_13B, ONLINE_EXPERIMENTS_META_OPT_7B, ONLINE_EXPERIMENTS_META_LLAMA_2_7B],\n",
    "    [\"opt-13b\", \"Llama-2-13b\", \"opt-6.7b\", \"Llama-2-7b\"]):\n",
    "    for dataset, meta in online_experiments_meta.items():\n",
    "        scenarios = meta[\"scenarios\"]\n",
    "\n",
    "        for scenario in scenarios:\n",
    "            num_of_blocks = scenario[\"total_blocks\"]\n",
    "            RESULTS_DIR = os.path.abspath(os.path.join(ROOT_DIR, \"results\", f\"online-{num_of_blocks}\"))\n",
    "\n",
    "            for req_rate in scenario[\"req_rates\"]:\n",
    "                input_dir = os.path.join(RESULTS_DIR, dataset, model, f\"n{N}\", f\"block{BLOCK_SIZE}\",\n",
    "                                        f\"req-rate-{req_rate}\", f\"seed{SEED}\", f\"duration-{DURATION}\"\n",
    "                                        )\n",
    "                requests_path = os.path.join(input_dir, \"requests.pkl\")\n",
    "                with open(requests_path, \"rb\") as f:\n",
    "                    requests = pickle.load(f)\n",
    "            \n",
    "                per_req_norm_latencies = []\n",
    "                for req in requests:\n",
    "                    arrival_time = req[\"arrival_time\"]\n",
    "                    finish_time = req[\"finish_time\"]\n",
    "                    output_len = len(req[\"output\"][\"outputs\"][0][\"token_ids\"])\n",
    "\n",
    "                    latency = finish_time - arrival_time\n",
    "                    norm_latency = latency / output_len\n",
    "                    per_req_norm_latencies.append(norm_latency)\n",
    "\n",
    "                # Average normalized latency\n",
    "                avg_latency = np.mean(per_req_norm_latencies)\n",
    "\n",
    "                # P95 normalized latency\n",
    "                p95_latency = np.percentile(per_req_norm_latencies, 95)\n",
    "\n",
    "                scenario[\"req_rates\"][req_rate] = {\n",
    "                    \"avg\": avg_latency,\n",
    "                    \"p95\": p95_latency\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(RES_DIR + \"online_experiments_meta_opt_13b.pkl\", 'wb') as f:\n",
    "    pickle.dump(ONLINE_EXPERIMENTS_META_OPT_13B, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(RES_DIR + \"online_experiments_llama_2_13b.pkl\", 'wb') as f:\n",
    "    pickle.dump(ONLINE_EXPERIMENTS_META_LLAMA_2_13B, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(RES_DIR + \"online_experiments_opt_6.7b.pkl\", 'wb') as f:\n",
    "    pickle.dump(ONLINE_EXPERIMENTS_META_OPT_7B, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(RES_DIR + \"online_experiments_llama_2_7b.pkl\", 'wb') as f:\n",
    "    pickle.dump(ONLINE_EXPERIMENTS_META_LLAMA_2_7B, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metadata import (\n",
    "    OFFLINE_EXPERIMENTS_META_OPT_13B,\n",
    "    OFFLINE_EXPERIMENTS_META_LLAMA_2_13B,\n",
    "    OFFLINE_EXPERIMENTS_META_OPT_7B,\n",
    "    OFFLINE_EXPERIMENTS_META_LLAMA_2_7B\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "BLOCK_SIZE = 16\n",
    "N = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for online_experiments_meta, model in zip(\n",
    "    [OFFLINE_EXPERIMENTS_META_OPT_13B, OFFLINE_EXPERIMENTS_META_LLAMA_2_13B, OFFLINE_EXPERIMENTS_META_OPT_7B, OFFLINE_EXPERIMENTS_META_LLAMA_2_7B],\n",
    "    [\"opt-13b\", \"Llama-2-13b\", \"opt-6.7b\", \"Llama-2-7b\"]):\n",
    "    for dataset, meta in online_experiments_meta.items():\n",
    "        scenarios = meta[\"scenarios\"]\n",
    "\n",
    "        for scenario in scenarios:\n",
    "            num_of_blocks = scenario[\"total_blocks\"]\n",
    "            RESULTS_DIR = os.path.abspath(os.path.join(ROOT_DIR, \"results\", f\"offline-{num_of_blocks}\"))\n",
    "\n",
    "            for num_of_requests in scenario[\"num_of_requests\"]:\n",
    "                input_dir = os.path.join(RESULTS_DIR, dataset, model, f\"n{N}\", f\"block{BLOCK_SIZE}\",\n",
    "                                        f\"num-requests-{num_of_requests}\", f\"seed{SEED}\"\n",
    "                                        )\n",
    "                requests_path = os.path.join(input_dir, \"requests.pkl\")\n",
    "                with open(requests_path, \"rb\") as f:\n",
    "                    requests = pickle.load(f)\n",
    "\n",
    "                start_time = sorted(requests, key=lambda d: d[\"arrival_time\"])[0][\"arrival_time\"]\n",
    "                end_time = sorted(requests, key=lambda d: d[\"finish_time\"])[-1][\"finish_time\"]\n",
    "                \n",
    "                elapsed_time = end_time - start_time\n",
    "                total_num_tokens = 0\n",
    "                for request in requests:\n",
    "                    prompt_len = len(request[\"output\"][\"prompt_token_ids\"])\n",
    "\n",
    "                    output_len = 0\n",
    "                    for completion_output in request[\"output\"][\"outputs\"].values():\n",
    "                        output_len += len(completion_output[\"token_ids\"])\n",
    "\n",
    "                    total_num_tokens += prompt_len + output_len\n",
    "\n",
    "                request_throughput = len(requests) / elapsed_time\n",
    "                token_throughput = total_num_tokens / elapsed_time\n",
    "\n",
    "                scenario[\"num_of_requests\"][num_of_requests] = {\n",
    "                    \"request\": request_throughput,\n",
    "                    \"token\": token_throughput\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(RES_DIR + \"offline_experiments_meta_opt_13b.pkl\", 'wb') as f:\n",
    "    pickle.dump(OFFLINE_EXPERIMENTS_META_OPT_13B, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(RES_DIR + \"offline_experiments_meta_llama_2_13b.pkl\", 'wb') as f:\n",
    "    pickle.dump(OFFLINE_EXPERIMENTS_META_LLAMA_2_13B, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(RES_DIR + \"offline_experiments_meta_opt_6.7b.pkl\", 'wb') as f:\n",
    "    pickle.dump(OFFLINE_EXPERIMENTS_META_OPT_7B, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(RES_DIR + \"offline_experiments_meta_llama_2_7b.pkl\", 'wb') as f:\n",
    "    pickle.dump(OFFLINE_EXPERIMENTS_META_LLAMA_2_7B, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(RES_DIR + \"online_experiments_meta_opt_13b.pkl\", \"rb\") as f:\n",
    "    ONLINE_EXPERIMENTS_META_OPT_13B = pickle.load(f)\n",
    "\n",
    "with open(RES_DIR + \"online_experiments_llama_2_13b.pkl\", \"rb\") as f:\n",
    "    ONLINE_EXPERIMENTS_META_LLAMA_2_13B = pickle.load(f)\n",
    "\n",
    "with open(RES_DIR + \"online_experiments_opt_6.7b.pkl\", \"rb\") as f:\n",
    "    ONLINE_EXPERIMENTS_META_OPT_7B = pickle.load(f)\n",
    "\n",
    "with open(RES_DIR + \"online_experiments_llama_2_7b.pkl\", \"rb\") as f:\n",
    "    ONLINE_EXPERIMENTS_META_LLAMA_2_7B = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(RES_DIR + \"offline_experiments_meta_opt_13b.pkl\", \"rb\") as f:\n",
    "    OFFLINE_EXPERIMENTS_META_OPT_13B = pickle.load(f)\n",
    "\n",
    "with open(RES_DIR + \"offline_experiments_meta_llama_2_13b.pkl\", \"rb\") as f:\n",
    "    OFFLINE_EXPERIMENTS_META_LLAMA_2_13B = pickle.load(f)\n",
    "\n",
    "with open(RES_DIR + \"offline_experiments_meta_opt_6.7b.pkl\", \"rb\") as f:\n",
    "    OFFLINE_EXPERIMENTS_META_OPT_7B = pickle.load(f)\n",
    "\n",
    "with open(RES_DIR + \"offline_experiments_meta_llama_2_7b.pkl\", \"rb\") as f:\n",
    "    OFFLINE_EXPERIMENTS_META_LLAMA_2_7B = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axis_title_size = 22\n",
    "label_size = 22\n",
    "legend_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 words per second, 1 token is approx 3/4 word, 5.333 token/s, SLO is 0.1875s/token\n",
    "SLO = 0.1875\n",
    "METRIC = \"p95\"\n",
    "THROUGHPUT_METRIC = \"request\"\n",
    "NUM_OF_REQUESTS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams[\"hatch.color\"] = \"#141414\"\n",
    "mpl.rcParams[\"hatch.linewidth\"] = 3.5\n",
    "hatches = [\"\", \"//\"]\n",
    "\n",
    "x = [\"Alpaca\", \"CNN DailyMail\", \"Dolly\", \"ShareGPT\"]\n",
    "labels = [\"OPT-13B\", \"Lllama-2-13B\"]\n",
    "w = 0.4\n",
    "l = 0.05\n",
    "L = 1\n",
    "\n",
    "ticks = get_tick_positions(x, labels, w, l , L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[6.4, 4.8])\n",
    "\n",
    "idx = 0\n",
    "for online_experiments_meta in [ONLINE_EXPERIMENTS_META_OPT_13B, ONLINE_EXPERIMENTS_META_LLAMA_2_13B]:\n",
    "    max_request_rates = []\n",
    "    for dataset, meta in online_experiments_meta.items():\n",
    "        scenarios = meta[\"scenarios\"]\n",
    "\n",
    "        for scenario in scenarios:\n",
    "            if scenario[\"total_blocks\"] != 643:\n",
    "                continue\n",
    "            \n",
    "            req_rates = sorted(scenario[\"req_rates\"])\n",
    "            max_req_rate = 0\n",
    "            for i in range(len(req_rates)):\n",
    "                if i in [0, 1]:\n",
    "                    continue\n",
    "\n",
    "                if scenario[\"req_rates\"][req_rates[i]][METRIC] > SLO and scenario[\"req_rates\"][req_rates[i-1]][METRIC] > SLO:\n",
    "                    if scenario[\"req_rates\"][req_rates[i-2]][METRIC] > SLO:\n",
    "                        max_req_rate = 0\n",
    "                    else:\n",
    "                        max_req_rate = req_rates[i-2]\n",
    "                    break\n",
    "            \n",
    "            max_request_rates.append(max_req_rate)\n",
    "\n",
    "    bar_colors = [dataset[\"color\"] for dataset in online_experiments_meta.values()]\n",
    "\n",
    "    ax.bar(ticks[idx], max_request_rates, w, color=bar_colors, label=labels[idx], hatch=hatches[idx])\n",
    "    idx = idx + 1\n",
    "\n",
    "ax.set_ylabel(\"Max Request Rate (req/s)\", size=axis_title_size)\n",
    "ax.set_yticks([0, 5, 10, 15, 20, 25, 30])\n",
    "ax.tick_params(axis=\"y\", labelsize=label_size)\n",
    "\n",
    "ax.set_xticks([])\n",
    "\n",
    "ax.legend(fontsize=legend_size)\n",
    "\n",
    "leg = ax.get_legend()\n",
    "leg.legend_handles[0].set_facecolor('white')\n",
    "leg.legend_handles[0].set_edgecolor('#141414')\n",
    "leg.legend_handles[1].set_facecolor('white')\n",
    "leg.legend_handles[1].set_edgecolor('#141414')\n",
    "\n",
    "ax.text(0.625, 0.75, \"0.5\", fontsize=16)\n",
    "ax.text(1.0125, 0.25, \"0.02\", fontsize=16)\n",
    "\n",
    "file_path = os.path.join(ROOT_DIR, \"figures\", \"figure_2a.pdf\")\n",
    "\n",
    "fig.savefig(file_path, dpi=300, bbox_inches=\"tight\", format=\"pdf\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[6.4, 4.8])\n",
    "\n",
    "idx = 0\n",
    "for offline_experiments_meta in [OFFLINE_EXPERIMENTS_META_OPT_13B, OFFLINE_EXPERIMENTS_META_LLAMA_2_13B]:\n",
    "    throughputs = []\n",
    "    for dataset, meta in offline_experiments_meta.items():\n",
    "        scenarios = meta[\"scenarios\"]\n",
    "\n",
    "        for scenario in scenarios:\n",
    "            if scenario[\"total_blocks\"] != 643:\n",
    "                continue\n",
    "\n",
    "            throughputs.append(scenario[\"num_of_requests\"][NUM_OF_REQUESTS][THROUGHPUT_METRIC])\n",
    "\n",
    "    bar_labels = [dataset[\"name\"] for dataset in offline_experiments_meta.values()]\n",
    "    bar_colors = [dataset[\"color\"] for dataset in offline_experiments_meta.values()]\n",
    "\n",
    "    bars = ax.bar(ticks[idx], throughputs, w, color=bar_colors, label=labels[idx], hatch=hatches[idx])\n",
    "    idx = idx + 1\n",
    "\n",
    "ax.set_ylabel(\"Throughput (req/s)\", size=axis_title_size)\n",
    "ax.tick_params(axis=\"y\", labelsize=label_size)\n",
    "\n",
    "ax.set_xticks([])\n",
    "\n",
    "ax.legend(fontsize=legend_size)\n",
    "\n",
    "leg = ax.get_legend()\n",
    "leg.legend_handles[0].set_facecolor('white')\n",
    "leg.legend_handles[0].set_edgecolor('#141414')\n",
    "leg.legend_handles[1].set_facecolor('white')\n",
    "leg.legend_handles[1].set_edgecolor('#141414')\n",
    "\n",
    "file_path = os.path.join(ROOT_DIR, \"figures\", \"figure_2b.pdf\")\n",
    "\n",
    "fig.savefig(file_path, dpi=300, bbox_inches=\"tight\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legendFig = plt.figure()\n",
    "\n",
    "dataset_names = [dataset[\"name\"] for dataset in OFFLINE_EXPERIMENTS_META_OPT_13B.values()]\n",
    "leg = legendFig.legend(\n",
    "    bars, dataset_names, loc=\"lower left\", fontsize=legend_size, ncols=4,\n",
    "    handlelength=2\n",
    ")\n",
    "\n",
    "for handle in leg.legend_handles:\n",
    "    handle.set_hatch(\"\")\n",
    "\n",
    "file_path = os.path.join(ROOT_DIR, \"figures\", \"legend_figure_2.pdf\")\n",
    "\n",
    "legendFig.savefig(file_path, dpi=300, bbox_inches=\"tight\", format=\"pdf\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 3a (Request Rate under SLO per Number of Blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams[\"hatch.color\"] = \"#000000\"\n",
    "mpl.rcParams[\"hatch.linewidth\"] = 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axis_title_size = 28\n",
    "label_size = 28\n",
    "legend_size = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 words per second, 1 token is approx 3/4 word, 5.333 token/s, SLO is 0.1875s/token\n",
    "SLO = 0.1875\n",
    "METRIC = \"p95\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 2\n",
    "fig, axs = plt.subplots(2, 2, figsize=[scale*6.4, scale*4.8], sharex=True)\n",
    "\n",
    "for dataset, dataset_name, ax in zip([\"alpaca\", \"cnn_dailymail\", \"dolly\", \"sharegpt\"], [\"Alpaca\", \"CNN DailyMail\", \"Dolly\", \"ShareGPT\"], axs.flat):\n",
    "    x = [\"OPT\", \"Llama-2\"]\n",
    "    num_of_total_blocks = [150, 300, 450, 600]\n",
    "    w = 0.175\n",
    "    l = 0.05\n",
    "\n",
    "    L = 1\n",
    "\n",
    "    positions = get_tick_positions(x, num_of_total_blocks, w, l, L)\n",
    "\n",
    "    label_names = [r\"1$\\times$\", r\"2$\\times$\", r\"3$\\times$\", r\"4$\\times$\"]\n",
    "    colors = [\"#cae6f4\", \"#9ad5f5\", \"#64c1f4\", \"#3fa7e7\"]\n",
    "\n",
    "    opt_bars = []\n",
    "    llama_bars = []\n",
    "    block_bars = []\n",
    "    for idx, total_blocks in enumerate(num_of_total_blocks):\n",
    "        max_request_rates = []\n",
    "        for ONLINE_METADATA, TITLE, suffix in zip(\n",
    "            [ONLINE_EXPERIMENTS_META_OPT_7B, ONLINE_EXPERIMENTS_META_LLAMA_2_7B],\n",
    "            [\"OPT-6.7B\", \"Llama-2-7B\"],\n",
    "            [\"opt_6.7b\", \"llama_2_7b\"]\n",
    "        ):\n",
    "            \n",
    "            scenarios = ONLINE_METADATA[dataset][\"scenarios\"]\n",
    "\n",
    "            for scenario in scenarios:\n",
    "                if scenario[\"total_blocks\"] != total_blocks:\n",
    "                    continue\n",
    "\n",
    "                req_rates = sorted(scenario[\"req_rates\"])\n",
    "                max_req_rate = 0\n",
    "                for i in range(len(req_rates)):\n",
    "                    if i in [0, 1]:\n",
    "                        continue\n",
    "\n",
    "                    if scenario[\"req_rates\"][req_rates[i]][METRIC] > SLO and scenario[\"req_rates\"][req_rates[i-1]][METRIC] > SLO:\n",
    "                        if scenario[\"req_rates\"][req_rates[i-2]][METRIC] > SLO:\n",
    "                            max_req_rate = 0\n",
    "                        else:\n",
    "                            max_req_rate = req_rates[i-2]\n",
    "                        break\n",
    "                \n",
    "                max_request_rates.append(max_req_rate)\n",
    "\n",
    "        if total_blocks not in [150, 300]:\n",
    "            label = \"_nolegend_\"\n",
    "        elif total_blocks == 150:\n",
    "            label = \"OPT-6.7B\"\n",
    "        else:\n",
    "            label = \"Llama-2-7B\"\n",
    "\n",
    "        bar = ax.bar(positions[idx], max_request_rates, w, color=colors[idx], label=label)\n",
    "\n",
    "        if total_blocks == 150:\n",
    "            opt_bars.append(bar)\n",
    "\n",
    "        if total_blocks == 300:\n",
    "            llama_bars.append(bar)\n",
    "\n",
    "        block_bars.append(bar)\n",
    "\n",
    "        max_request_rates = []\n",
    "        for ONLINE_METADATA, TITLE, suffix in zip(\n",
    "            [ONLINE_EXPERIMENTS_META_OPT_13B, ONLINE_EXPERIMENTS_META_LLAMA_2_13B],\n",
    "            [\"OPT-13B\", \"Llama-2-13B\"],\n",
    "            [\"opt_13b\", \"llama_2_13b\"]\n",
    "        ):\n",
    "            \n",
    "            scenarios = ONLINE_METADATA[dataset][\"scenarios\"]\n",
    "\n",
    "            for scenario in scenarios:\n",
    "                if scenario[\"total_blocks\"] != total_blocks:\n",
    "                    continue\n",
    "\n",
    "                req_rates = sorted(scenario[\"req_rates\"])\n",
    "                max_req_rate = 0\n",
    "                for i in range(len(req_rates)):\n",
    "                    if i in [0, 1]:\n",
    "                        continue\n",
    "\n",
    "                    if scenario[\"req_rates\"][req_rates[i]][METRIC] > SLO and scenario[\"req_rates\"][req_rates[i-1]][METRIC] > SLO:\n",
    "                        if scenario[\"req_rates\"][req_rates[i-2]][METRIC] > SLO:\n",
    "                            max_req_rate = 0\n",
    "                        else:\n",
    "                            max_req_rate = req_rates[i-2]\n",
    "                        break\n",
    "                \n",
    "                max_request_rates.append(max_req_rate)\n",
    "        \n",
    "        if total_blocks not in (150, 300):\n",
    "            label = \"_nolegend_\"\n",
    "        elif total_blocks == 150:\n",
    "            label = \"OPT-13B\"\n",
    "        else:\n",
    "            label = \"Llama-2-13B\"\n",
    "\n",
    "        bar = ax.bar(positions[idx], max_request_rates, w, color=colors[idx], label=label, hatch=\"//\", alpha=.99)\n",
    "\n",
    "        if total_blocks == 150:\n",
    "            opt_bars.append(bar)\n",
    "\n",
    "        if total_blocks == 300:\n",
    "            llama_bars.append(bar)\n",
    "\n",
    "    ax.tick_params(axis=\"y\", labelsize=label_size)\n",
    "\n",
    "    if dataset == \"alpaca\":\n",
    "        ax.set_yticks([0, 10, 20, 30, 40, 50])\n",
    "        ax.set_ylim(0, 51)\n",
    "    elif dataset in [\"cnn_dailymail\", \"sharegpt\"]:\n",
    "        ax.set_yticks([0, 2, 4, 6])\n",
    "        ax.set_ylim(0, 6.12)\n",
    "    else:\n",
    "        ax.set_yticks([0, 10, 20, 30])\n",
    "        ax.set_ylim(0, 30.6)\n",
    "\n",
    "    ax.set_xticks([0, 1])\n",
    "    ax.set_xticklabels([\"OPT\", \"Llama-2\"])\n",
    "    ax.tick_params(axis=\"x\", labelsize=label_size)\n",
    "\n",
    "    ax.set_title(dataset_name, fontsize=axis_title_size)\n",
    "\n",
    "fig.text(0.025, 0.5, \"Max Request Rate (req/s)\", size=axis_title_size+4, va='center', rotation='vertical')\n",
    "\n",
    "file_path = os.path.join(ROOT_DIR, \"figures\", \"figure_3a.pdf\")\n",
    "\n",
    "fig.savefig(file_path, dpi=300, bbox_inches=\"tight\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 3b (Throughput per Number of Blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams[\"hatch.color\"] = \"#000000\"\n",
    "mpl.rcParams[\"hatch.linewidth\"] = 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axis_title_size = 28\n",
    "label_size = 28\n",
    "legend_size = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THROUGHPUT_METRIC = \"request\"\n",
    "NUM_OF_REQUESTS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 2\n",
    "fig, axs = plt.subplots(2, 2, figsize=[scale*6.4, scale*4.8], sharex=True)\n",
    "\n",
    "for dataset, dataset_name, ax in zip([\"alpaca\", \"cnn_dailymail\", \"dolly\", \"sharegpt\"], [\"Alpaca\", \"CNN DailyMail\", \"Dolly\", \"ShareGPT\"], axs.flat):\n",
    "    x = [\"OPT\", \"Llama-2\"]\n",
    "    num_of_total_blocks = [150, 300, 450, 600]\n",
    "    w = 0.175\n",
    "    l = 0.05\n",
    "\n",
    "    L = 1\n",
    "\n",
    "    positions = get_tick_positions(x, num_of_total_blocks, w, l, L)\n",
    "\n",
    "    label_names = [r\"1$\\times$\", r\"2$\\times$\", r\"3$\\times$\", r\"4$\\times$\"]\n",
    "    colors = [\"#cae6f4\", \"#9ad5f5\", \"#64c1f4\", \"#3fa7e7\"]\n",
    "\n",
    "    opt_bars = []\n",
    "    llama_bars = []\n",
    "    block_bars = []\n",
    "    for idx, total_blocks in enumerate(num_of_total_blocks):\n",
    "        throughputs = []\n",
    "        for OFFLINE_METADATA, TITLE, suffix in zip(\n",
    "            [OFFLINE_EXPERIMENTS_META_OPT_7B, OFFLINE_EXPERIMENTS_META_LLAMA_2_7B],\n",
    "            [\"OPT-6.7B\", \"Llama-2-7B\"],\n",
    "            [\"opt_6.7b\", \"llama_2_7b\"]\n",
    "        ):\n",
    "            \n",
    "            scenarios = OFFLINE_METADATA[dataset][\"scenarios\"]\n",
    "\n",
    "            for scenario in scenarios:\n",
    "                if scenario[\"total_blocks\"] != total_blocks:\n",
    "                    continue\n",
    "\n",
    "                throughputs.append(scenario[\"num_of_requests\"][NUM_OF_REQUESTS][THROUGHPUT_METRIC])\n",
    "\n",
    "        if total_blocks not in [150, 300]:\n",
    "            label = \"_nolegend_\"\n",
    "        elif total_blocks == 150:\n",
    "            label = \"OPT-6.7B\"\n",
    "        else:\n",
    "            label = \"Llama-2-7B\"\n",
    "\n",
    "        bar = ax.bar(positions[idx], throughputs, w, color=colors[idx], label=label)\n",
    "\n",
    "        if total_blocks == 150:\n",
    "            opt_bars.append(bar)\n",
    "\n",
    "        if total_blocks == 300:\n",
    "            llama_bars.append(bar)\n",
    "\n",
    "        block_bars.append(bar)\n",
    "\n",
    "        throughputs = []\n",
    "        for OFFLINE_METADATA, TITLE, suffix in zip(\n",
    "            [OFFLINE_EXPERIMENTS_META_OPT_13B, OFFLINE_EXPERIMENTS_META_LLAMA_2_13B],\n",
    "            [\"OPT-13B\", \"Llama-2-13B\"],\n",
    "            [\"opt_13b\", \"llama_2_13b\"]\n",
    "        ):\n",
    "            \n",
    "            scenarios = OFFLINE_METADATA[dataset][\"scenarios\"]\n",
    "\n",
    "            for scenario in scenarios:\n",
    "                if scenario[\"total_blocks\"] != total_blocks:\n",
    "                    continue\n",
    "\n",
    "                throughputs.append(scenario[\"num_of_requests\"][NUM_OF_REQUESTS][THROUGHPUT_METRIC])\n",
    "        \n",
    "        if total_blocks not in (150, 300):\n",
    "            label = \"_nolegend_\"\n",
    "        elif total_blocks == 150:\n",
    "            label = \"OPT-13B\"\n",
    "        else:\n",
    "            label = \"Llama-2-13B\"\n",
    "\n",
    "        bar = ax.bar(positions[idx], throughputs, w, color=colors[idx], label=label, hatch=\"//\", alpha=.99)\n",
    "\n",
    "        if total_blocks == 150:\n",
    "            opt_bars.append(bar)\n",
    "\n",
    "        if total_blocks == 300:\n",
    "            llama_bars.append(bar)\n",
    "\n",
    "    ax.tick_params(axis=\"y\", labelsize=label_size)\n",
    "\n",
    "    if dataset == \"alpaca\":\n",
    "        ax.set_yticks([0, 10, 20, 30, 40, 50])\n",
    "        ax.set_ylim(0, 54)\n",
    "    elif dataset in [\"cnn_dailymail\", \"sharegpt\"]:\n",
    "        ax.set_yticks([0, 2, 4, 6, 8])\n",
    "        ax.set_ylim(0, 8.64)\n",
    "    else:\n",
    "        ax.set_yticks([0, 10, 20, 30])\n",
    "        ax.set_ylim(0, 32.4)\n",
    "\n",
    "    ax.set_xticks([0, 1])\n",
    "    ax.set_xticklabels([\"OPT\", \"Llama-2\"])\n",
    "    ax.tick_params(axis=\"x\", labelsize=label_size)\n",
    "\n",
    "    ax.set_title(dataset_name, fontsize=axis_title_size)\n",
    "\n",
    "fig.text(0.025, 0.5, \"Throughput (req/s)\", size=axis_title_size+4, va='center', rotation='vertical')\n",
    "\n",
    "file_path = os.path.join(ROOT_DIR, \"figures\", \"figure_3b.pdf\")\n",
    "\n",
    "fig.savefig(file_path, dpi=300, bbox_inches=\"tight\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale=4.75\n",
    "scale=2.25\n",
    "legendFig, axs = plt.subplots(2, 1, figsize=[scale*6.4, 3], gridspec_kw = {'wspace':0, 'hspace':0})\n",
    "\n",
    "handles = opt_bars + llama_bars\n",
    "labels = [\"OPT-6.7B\", \"OPT-13B\", \"Llama-2-7B\", \"Llama-2-13B\"]\n",
    "\n",
    "axs[0].axis(\"off\")\n",
    "leg_1 = axs[0].legend(\n",
    "    handles, labels, loc=\"lower center\", fontsize=legend_size, ncols=4,\n",
    "    handlelength=1.5, columnspacing=1.5, mode=\"expand\", handletextpad=0.6,\n",
    "    title=\"Models\", title_fontsize=legend_size\n",
    ")\n",
    "\n",
    "for handle in leg_1.legend_handles:\n",
    "    handle.set_facecolor('white')\n",
    "    handle.set_edgecolor('#141414')\n",
    "\n",
    "axs[1].axis(\"off\")\n",
    "leg_2 = axs[1].legend(\n",
    "    block_bars, label_names, loc=\"lower center\", fontsize=legend_size, ncols=4,\n",
    "    handlelength=2, mode=\"expand\",\n",
    "    title=\"Memory\", title_fontsize=legend_size\n",
    ")\n",
    "\n",
    "for handle in leg_2.legend_handles:\n",
    "    handle.set_edgecolor('#141414')\n",
    "\n",
    "file_path = os.path.join(ROOT_DIR, \"figures\", \"legend_figure_3.pdf\")\n",
    "\n",
    "legendFig.savefig(file_path, dpi=300, bbox_inches=\"tight\", format=\"pdf\")\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
